# **생선 분류 문제**

### **1. 도미 데이터 준비하기**
* class(클래스): 종류
* classification(분류): 여러 클래스 중 하나를 구별해 내는 문제
* binary classification(이진분류): 2개의 클래스 중 하나를 고르는 문제 *e.g.도미와 빙어 구분하기*
* feature(특성): 데이터의 특징 e.g.도미의 *길이와 무게*
* scatter plot(산점도): x, y축으로 이뤄진 좌표계에 두 변수 (x, y)의 관계를 표현하는 방법
* matplotlib(맷플롯립): 파이썬에서 과학계산용 그래프를 그리는 대표적인 패키지
* scatter() 함수: 맷플롯립을 임포트하고 산점도를 그리는 함수
* import(임포트): 따로 만들어둔 파이썬 패키지(클래스와 함수의 묶음, e.g. matplotlib)을 사용하기 위해 불러오는 명령어
* 패키지: 기능을 구현한 함수를 특정 기능별로 묶어둔 것, 코랩에서는 널리 사용되는 파이썬 과학 패키지들을 미리 준비해두었기에 따로 설치할 필요가 없다
* as 키워드: 로 함수 이름을 줄여 씀. 관용적 줄임말 알아두기 e.g. pyplot as plt
* xlabel(), ylabel(): 각각 x축과 y축의 이름을 화면에 표시한다.
* show(): 준비된 그래프를 화면에 출력한다.

 도미 35마리를 2차원 그래프에 점으로 나타냈다. x축은 길이, y축은 무게이다.
 * 2개의 특성을 사용해 그린 그래프이기 때문에 2차원 그래프라고 말한다.생선의 길이가 길수록 무게가 많이 나간다고 생각하면 이 그래프의 모습은 매우 자연스럽다. 

* linear(선형): 산점도 그래프가 일직선에 가까운 형태로 나타나는 경우를 일컬어 이렇게 말한다.
---
### **2. 빙어 데이터 준비하기**
* 물류 센터에는 빙어가 많지 않아 준비한 빙어는 14마리이다.
* 빙어는 크기도 작고 무게도 가볍다.
* 맷플롭립에서 2개의 산점도를 한 그래프로 그리는 것은 아주 간단하다: scatter()함수 연달아 사용하기.
* 파이썬에서 한번 임포트된 패키지는 그 파일 안에서는 다시 임포트하지 않아도 된다. 코랩 노트북도 마찬가지이다. 하지만 구글 클라우드와 연결이 끊긴 후 다시 연결하면 패키지를 새로 임포트해야 한다.
* 친절하게 2개의 산점도를 색깔로 구분해준다. 주황색 점이 빙어이다. 빙어는 도미에 비해 길이도 무게도 작다.
* 빙어도 도미와 비슷하게 길이와 무게가 비례하지만 늘어나는 정도가 조금 다르다. 빙어는 길이가 늘어나더라도 무게가 많이 늘지 않습니다.
* 따라서 빙어의 산점도도 선형적이지만 무게가 길이에 영향을 덜 받는다고 볼 수 있다.
---

# **첫 번째 머신러닝 프로그램**
---
* k-Nearest Neighbors(k-최근접 이웃) 알고리즘을 사용해 도미와 빙어 데이터를 구분해보자!
* 두 리스트를 더해 하나의 리스트로: 앞에서 준비했던 도미와 빙어 데이터를 하나의 데이터로 합친다.
* e.g. length = [25.4, 26.3, .., 41.0,(도미 35개의 길이), 9.8, ... , 15.0, (빙어 14개의 길이)]
* scikit-learn(사이킷런): 머신러닝 패키지이며 각 특성의 리스트를 세로 방향으로 늘어뜨린 2차원 리스트를 만들어야 사용할 수 있다.
* 파이썬 zip()함수 & list comprehension(리스트 내포)구문: 2차원 리스트 만들기
* zip()함수: 나열된 리스트 각각에서 하나씩 원소를 꺼내 반환한다.
* 리스트 안에 각 샘플이 놓인다.
* k-Nearest Neighbors(k-NN, k-최근접 이웃)
K-최근접 이웃(K-NN, K-Nearest Neighbor) 알고리즘은 가장 간단한 머신러닝 알고리즘으로, 분류(Classification)알고리즘이다. 비슷한 특성을 가진 데이터는 비슷한 범주에 속하는 경향이 있다는 가정하에 사용한다.
주변의 가장 가까운 K개의 데이터를 보고 데이터가 속할 그룹을 판단하는 알고리즘이 K-NN 알고리즘이다.K-NN 알고리즘은, 단순히 훈련 데이터셋을 그냥 저장하는 것이 모델을 만드는 과정의 전부이다.거리를 측정할 땐 유클리드 거리(Euclidean distance)를 사용한다.
* 유클리드 거리: 유클리드 거리는 다차원 공간에서 두 점 사이의 직선 거리로 계산됩니다. 피타고라스의 정리를 기반으로 하며 두 벡터의 해당 요소 간 차이의 제곱합의 제곱근을 취하여 계산됩니다. 다차원 공간에서 두 지점 사이의 최단 경로 길이의 척도로 볼 수 있습니다.
* K-NN 알고리즘의 특징 중 하나는 K의 값에 따라 분류가 달라질 수 있다는 점이다.
* 항상 분류가 가능하도록 K는 홀수로 설정하는 것이 좋으며, 최선의 K를 선택하는 것은 데이터마다 다르게 접근해야 하는데, 일반적으로는 총 데이터 수의 제곱근 값을 사용한다.너무나 간단한 알고리즘이지만, 실제로 이미지 처리, 글자/얼굴 인식, 추천 알고리즘, 의료 분야 등에서 많이 사용된다. 그렇다면 장단점은 뭘까?
* 단순하기 때문에 다른 알고리즘에 비해 구현하기가 쉽다. 또 훈련 데이터를 그대로 가지고 있어 특별한 훈련을 하지 않기 때문에 훈련 단계가 매우 빠르게 수행된다.
* 단점으로는 모델을 생성하지 않기 때문에 특징과 클래스 간 관계를 이해하는데 제한적이다. 모델의 결과를 가지고 해석하는 것이 아니라, 미리 변수와 클래스 간의 관계를 파악하여 이를 알고리즘에 적용해야 원하는 결과를 얻을 수 있기 때문이다.
또, 적절한 K의 선택이 필요하고, 훈련 단계가 빠른 대신 데이터가 많아지면 분류 단계가 느리다.
* 임의의 생선이 주어질 때 해당 생선이 도미인지 빙어인지 구분하기 위해선 적어도 기존의 생선들이 도미인지 빙어인지를 알려주어야 하기 때문이다. 따라서 도미를 1로, 빙어를 0으로 나타낸다.
* 그다음엔 사이킷런 패키지에서 이미 구현되어 있는 K-NN 알고리즘 클래스인 KNeighborsClassifier를 import 한다. 그리고 해당 클래스의 객체를 만들어준다.
* 이 객체에 fish_data와 fish_target을 전달해서 도미를 찾기 위한 기준을 학습시킨다. 사이킷런에서 fit() 메서드가 이를 수행하며 이 과정을 훈련이라고 한다.
* 훈련이 잘 되었는지 평가하기 위해서는 score() 메서드를 사용한다. 0에서 1 사이의 값을 반환하고, 1은 모든 데이터를 정확히 맞혔다는 의미이고, 0.5는 절반의 데이터를 맞혔다는 의미이다.
* score 메서드를 수행하면 결과로 1.0이 나오게 된다. 즉, 올바르게 결괏값이 나왔기 때문에 도미와 빙어를 잘 분류했다고 볼 수 있다.
* 기본적으로 KNeighborsClassifier 클래스는 K의 기본값이 5로 설정되어있다. 이 기준은 n_neighbors 매개 변수로 변경할 수 있다. 만약 총 생선의 수인 49로 바꾸면 어떻게 될까?
